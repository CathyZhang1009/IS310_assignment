{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3bca650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import re\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "import gensim\n",
    "from stop_words import get_stop_words\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06dd76b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data\n",
    "data_list = os.listdir('./data')\n",
    "sum_list = []\n",
    "for i in data_list:\n",
    "    a = []\n",
    "    dir = './data'+'/'+i\n",
    "    with open(dir,'r',encoding = 'utf-8') as f:\n",
    "        content = f.read()\n",
    "    f.close()\n",
    "    a.append(i.split('.')[0])\n",
    "    a.append(content)\n",
    "    sum_list.append(a)\n",
    "\n",
    "df = pd.DataFrame(sum_list,columns = ['year','values'])\n",
    "df = df.set_index('year')\n",
    "value_data = df['values'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cd06192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "remove_pattern = [\n",
    "    (r'\\s+[a-zA-Z]\\s+', ' '),  # Remove all single characters\n",
    "    (r'\\^[a-zA-Z]\\s+', ' '),\n",
    "    (r'\\d+', ''),  # Remove all numbers\n",
    "    (r'\\s+', ' '),  # Substitute multiple white spaces with single space\n",
    "    (r'^b\\s+', ''),  # Remove prefixed 'b'\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde9e939",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_data = []\n",
    "\n",
    "for abstract in range(0, len(value_data)):\n",
    "    temp = unidecode.unidecode(re.sub(r'\\W', ' ', str(value_data[abstract])))  # Remove all the special characters\n",
    "    for p, r in remove_pattern:\n",
    "        temp = re.sub(p, r, temp)\n",
    "    word_data.append(temp)\n",
    "\n",
    "\n",
    "def sentence_to_words(abstract):\n",
    "    # If deacc=True then remove punctuations\n",
    "    return (gensim.utils.simple_preprocess(str(abstract), deacc=True))\n",
    "\n",
    "data_words = [sentence_to_words(x) for x in word_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aae672cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stop words list\n",
    "stop_words = get_stop_words('english')\n",
    "\n",
    "stop_words.extend(['actually', 'afterwards', 'almost', 'already', 'also', 'although', 'among', 'amongst', 'another', 'apart', 'around', 'aside', 'author', 'authors', 'away',\n",
    "                   'become', 'becoming', 'better', 'beyond', 'certain', 'could', 'commonly', 'considerable', 'consider', 'de', 'definitely', 'eg', 'either', 'etc',\n",
    "                   'f', 'g', 'h', 'hence', 'hereafter', 'herein', 'however', 'indeed', 'instead', 'illustrate', 'illustrates', 'demonstrate', 'demonstrates',\n",
    "                   'important',  'j', 'k', 'likely', 'many', 'may', 'maybe', 'meanwhile', 'might', 'moreover', 'much', 'n', 'nevertheless', 'neither', 'normally', 'often',\n",
    "                   'otherwise', 'particular', 'q', 'quite', 'understand', 'understood',\n",
    "                   'rather', 'regardless', 'relatively', 'respectively', 'reveal', 'since', 'suggest', 'suggests', 'specifically', 'particularly', 'therefore', 'therein',\n",
    "                   'though', 'thus', 'together', 'toward', 'towards', 'unless', 'upon', 'using', 'well', 'whereas', 'whereafter',\n",
    "                   'whether',  'amico', 'edt', 'amp','mccarty_at_kcl', 'est', 'know', 'willard',\n",
    "                   'htm',  'com', 'new', 'like', 'fqs',  'use', 'text', 'cch', 'bitnet', 'mccarty', 'kcl', 'html',\n",
    "                    'org', 'ac', 'uk', 'edu', 'num', 'vol', 'date', 'href', 'www', 'http', 'will', 'ninch'])\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    result = [x for x in words if x not in stop_words]\n",
    "    return result\n",
    "\n",
    "data_words_nostops = [remove_stopwords(x) for x in data_words] # Remove stop words\n",
    "\n",
    "test = [\" \".join(x) for x in data_words_nostops]\n",
    "\n",
    "sum_list = copy.deepcopy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44eb81eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('utorepas', 1329.120536739557), ('research', 1399.0), ('computer', 1969.0), ('information', 2006.0), ('can', 2288.0), ('humanities', 2299.0), ('one', 2404.0), ('university', 2443.0), ('subject', 3604.0), ('humanist', 4495.0)]\n",
      "[('subject', 1206.0), ('humanist', 1288.0), ('humanities', 1288.0), ('research', 1399.0), ('university', 1400.0)]\n",
      "[('subject', 1071.0), ('information', 1264.0), ('university', 1294.0), ('humanities', 1550.0), ('humanist', 2757.0)]\n",
      "[('humanities', 1132.0), ('information', 1143.0), ('subject', 1238.0), ('university', 1395.0), ('humanist', 1405.0)]\n",
      "[('can', 659.0), ('software', 660.0), ('university', 707.0), ('subject', 920.0), ('humanist', 1418.0)]\n",
      "[('one', 1209.0), ('can', 1322.0), ('university', 1892.0), ('subject', 2297.0), ('humanist', 2539.0)]\n",
      "[('university', 2178.0), ('can', 2288.0), ('one', 2404.0), ('subject', 3604.0), ('humanist', 4443.0)]\n",
      "[('wmccarty', 864.8123175954696), ('research', 970.0), ('digital', 1048.0), ('university', 1071.0), ('humanities', 1165.0)]\n",
      "[('information', 923.0), ('humanities', 926.0), ('subject', 1066.0), ('humanist', 1117.0), ('university', 1147.0)]\n",
      "[('subject', 1266.0), ('university', 1388.0), ('information', 1716.0), ('humanities', 2074.0), ('humanist', 4451.0)]\n",
      "[('can', 720.0), ('information', 813.0), ('subject', 1029.0), ('humanist', 1116.0), ('university', 1340.0)]\n",
      "[('research', 1164.0), ('humanities', 1267.0), ('subject', 1309.0), ('university', 1325.0), ('humanist', 1441.0)]\n",
      "[('subject', 1410.0), ('information', 2006.0), ('university', 2198.0), ('humanities', 2299.0), ('humanist', 4225.0)]\n",
      "[('one', 971.0), ('information', 1103.0), ('subject', 1776.0), ('humanist', 1882.0), ('university', 2327.0)]\n",
      "[('research', 859.0), ('information', 1157.0), ('humanist', 1350.0), ('subject', 1351.0), ('university', 2443.0)]\n",
      "[('information', 942.0), ('humanist', 1012.0), ('digital', 1207.0), ('university', 1465.0), ('humanities', 1538.0)]\n",
      "[('one', 880.0), ('can', 1048.0), ('subject', 1358.0), ('university', 1634.0), ('humanist', 1757.0)]\n",
      "[('university', 2061.0), ('can', 2218.0), ('one', 2299.0), ('subject', 3321.0), ('humanist', 4495.0)]\n",
      "[('conference', 613.0), ('information', 827.0), ('subject', 876.0), ('humanist', 888.0), ('university', 1510.0)]\n",
      "[('humanities', 1006.0), ('information', 1118.0), ('subject', 1164.0), ('humanist', 1287.0), ('university', 1310.0)]\n",
      "[('university', 1128.0), ('computer', 1174.0), ('subject', 1239.0), ('utorepas', 1329.120536739557), ('humanist', 2117.0)]\n",
      "[('can', 1010.0), ('information', 1318.0), ('subject', 1418.0), ('university', 1527.0), ('humanist', 1650.0)]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate tf instance\n",
    "model = TfidfVectorizer(use_idf=True, smooth_idf=True, norm=None)\n",
    "# Input training set matrix, each row represents one text\n",
    "model_fit = model.fit_transform(sum_list)\n",
    "word=model.get_feature_names()\n",
    "tfidf_matrix=model_fit.toarray()\n",
    "\n",
    "\n",
    "max_idf = tfidf_matrix.max(axis=0).tolist()\n",
    "s_d = [(word[i], max_idf[i]) for i in range(len(max_idf))]\n",
    "sortedList2 = sorted(s_d, key=lambda x:x[1])\n",
    "sd = sortedList2[-10:]\n",
    "print(sd)\n",
    "\n",
    "for id_text in range(0, tfidf_matrix.shape[0]):\n",
    "    rank = [i for i, v in sorted(list(enumerate(tfidf_matrix[id_text])), key=lambda x:x[1])]\n",
    "    s=[(word[kl], tfidf_matrix[id_text][kl]) for kl in rank[-5:]]\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d27d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
