{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e0e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import unidecode\n",
    "import gensim\n",
    "from stop_words import get_stop_words\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import altair as alt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c98fcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data\n",
    "data_list = os.listdir('./data')\n",
    "sum_list = []\n",
    "for i in data_list:\n",
    "    a = []\n",
    "    dir = './data'+'/'+i\n",
    "    with open(dir,'r',encoding = 'utf-8') as f:\n",
    "        content = f.read()\n",
    "    f.close()\n",
    "    a.append(i.split('.')[0])\n",
    "    a.append(content)\n",
    "    sum_list.append(a)\n",
    "\n",
    "df = pd.DataFrame(sum_list,columns = ['year','values'])\n",
    "df = df.set_index('year')\n",
    "time_list = df.index.values.tolist()\n",
    "value_data = df['values'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ecdeb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "remove_pattern = [\n",
    "    (r'\\s+[a-zA-Z]\\s+', ' '),  # Remove all single characters\n",
    "    (r'\\^[a-zA-Z]\\s+', ' '),\n",
    "    (r'\\d+', ''),  # Remove all numbers\n",
    "    (r'\\s+', ' '),  # Substitute multiple white spaces with single space\n",
    "    (r'^b\\s+', ''),  # Remove prefixed 'b'\n",
    "                  ]\n",
    "word_data = []\n",
    "\n",
    "for abstract in range(0, len(value_data)):\n",
    "    temp = unidecode.unidecode(re.sub(r'\\W', ' ', str(value_data[abstract])))  # Remove all the special characters\n",
    "    for p, r in remove_pattern:\n",
    "        temp = re.sub(p, r, temp)\n",
    "    word_data.append(temp)\n",
    "\n",
    "\n",
    "def sentence_to_words(abstract):\n",
    "    # If deacc=True then remove punctuations\n",
    "    return (gensim.utils.simple_preprocess(str(abstract), deacc=True))\n",
    "\n",
    "data_words = [sentence_to_words(x) for x in word_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07831b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get stop words list\n",
    "stop_words = get_stop_words('english')\n",
    "\n",
    "stop_words.extend(['actually', 'afterwards', 'almost', 'already', 'also', 'although', 'among', 'amongst', 'another', 'apart', 'around', 'aside', 'author', 'authors', 'away',\n",
    "                   'become', 'becoming', 'better', 'beyond', 'certain', 'could', 'commonly', 'considerable', 'consider', 'de', 'definitely', 'eg', 'either', 'etc',\n",
    "                   'f', 'g', 'h', 'hence', 'hereafter', 'herein', 'however', 'indeed', 'instead', 'illustrate', 'illustrates', 'demonstrate', 'demonstrates',\n",
    "                   'important',  'j', 'k', 'likely', 'many', 'may', 'maybe', 'meanwhile', 'might', 'moreover', 'much', 'n', 'nevertheless', 'neither', 'normally', 'often',\n",
    "                   'otherwise', 'particular', 'q', 'quite', 'understand', 'understood',\n",
    "                   'rather', 'regardless', 'relatively', 'respectively', 'reveal', 'since', 'suggest', 'suggests', 'specifically', 'particularly', 'therefore', 'therein',\n",
    "                   'though', 'thus', 'together', 'toward', 'towards', 'unless', 'upon', 'using', 'well', 'whereas', 'whereafter',\n",
    "                   'whether',  'amico', 'edt', 'amp','mccarty_at_kcl', 'est', 'know', 'willard',\n",
    "                   'htm',  'com', 'new', 'like', 'fqs',  'use', 'text', 'cch', 'bitnet', 'mccarty', 'kcl', 'html',\n",
    "                    'org', 'ac', 'uk', 'edu', 'num', 'vol', 'date', 'href', 'www', 'http', 'will', 'ninch'])\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    result = [x for x in words if x not in stop_words]\n",
    "    return result\n",
    "\n",
    "data_words_nostops = [remove_stopwords(x) for x in data_words] # Remove stop words\n",
    "\n",
    "test = [\" \".join(x) for x in data_words_nostops]\n",
    "\n",
    "sum_list = copy.deepcopy(test)\n",
    "a_list=sum_list[-8:]\n",
    "b_list=sum_list[:-8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "578bda06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper1(list1):\n",
    "    # Instantiate tf instance\n",
    "    model = TfidfVectorizer(use_idf=True, smooth_idf=True, norm=None)\n",
    "    # Input training set matrix, each row represents one text\n",
    "    model_fit = model.fit_transform(list1)\n",
    "    word=model.get_feature_names()\n",
    "    tfidf_matrix=model_fit.toarray()\n",
    "    max_idf = tfidf_matrix.max(axis=0).tolist()\n",
    "    s_d = [(word[i], max_idf[i]) for i in range(len(max_idf))]\n",
    "    sortedList2 = sorted(s_d, key=lambda x: x[1])\n",
    "    sd = sortedList2[-10:]\n",
    "    print(sd)\n",
    "    return word,tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04577519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('utorepas', 1329.120536739557), ('research', 1399.0), ('computer', 1969.0), ('information', 2006.0), ('can', 2288.0), ('humanities', 2299.0), ('one', 2404.0), ('university', 2443.0), ('subject', 3604.0), ('humanist', 4495.0)]\n",
      "[('digital', 1207.0), ('utorepas', 1246.4125319481634), ('information', 1318.0), ('humanities', 1538.0), ('computer', 1969.0), ('can', 2218.0), ('one', 2299.0), ('university', 2443.0), ('subject', 3321.0), ('humanist', 4495.0)]\n",
      "[('re', 1273.0), ('language', 1294.0), ('research', 1399.0), ('information', 2006.0), ('can', 2288.0), ('humanities', 2299.0), ('university', 2327.0), ('one', 2404.0), ('subject', 3604.0), ('humanist', 4451.0)]\n"
     ]
    }
   ],
   "source": [
    "but,buts = helper1(sum_list)\n",
    "but_a,buts_a=helper1(a_list)\n",
    "but_b,buts_b=helper1(b_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95deaf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper2(word,but,buts,time_list):\n",
    "    num=but.index(word)\n",
    "    q=[{'year':time_list[j],'value':buts[j][num]}for j in range(0,len(buts))]\n",
    "    df = pd.DataFrame(q)\n",
    "    alt.Chart(df).mark_line(point=True).encode(\n",
    "        x = 'year',\n",
    "        y = 'value'\n",
    "    )\n",
    "    \n",
    "helper2('sun',but,buts,time_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2569727a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
